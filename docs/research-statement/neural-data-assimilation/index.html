<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>A neural network model for data assimilation and uncertainty quantification | S.K.R</title>
<meta name=keywords content="Variational data assimilation,machine learning,neural networks,uncertainty quantification,AI,Deeplearning for weather predictions">
<meta name=description content="Can we perform data assimilation and uncertainty quantification using neural networks?">
<meta name=author content="Me">
<link rel=canonical href=https://shashankkroy.github.io/research-statement/neural-data-assimilation/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.f3a5fcf1a2ed223c5f9f0e42ad1a03f290d7f781cfb741dbede89bcb6f8be329.css integrity="sha256-86X88aLtIjxfnw5CrRoD8pDX94HPt0Hb7eiby2+L4yk=" rel="preload stylesheet" as=style>
<link rel=icon href=https://shashankkroy.github.io/favicon.ico>
<link rel=apple-touch-icon href=https://shashankkroy.github.io/apple-touch-icon.png>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-M628446BVJ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-M628446BVJ',{anonymize_ip:!1})}</script>
<meta name=twitter:card content="summary">
<meta name=twitter:title content="A neural network model for data assimilation and uncertainty quantification | S.K.R">
<meta name=twitter:description content="Can we perform data assimilation and uncertainty quantification using neural networks?">
<meta property="og:title" content="A neural network model for data assimilation and uncertainty quantification | S.K.R">
<meta property="og:description" content="Can we perform data assimilation and uncertainty quantification using neural networks?">
<meta property="og:type" content="article">
<meta property="og:url" content="https://shashankkroy.github.io/research-statement/neural-data-assimilation/">
<meta property="og:image" content="https://shashankkroy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta property="article:section" content="research statement">
<meta property="article:published_time" content="2025-07-22T00:00:00+00:00">
<meta property="article:modified_time" content="2025-07-22T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"A neural network model for data assimilation and uncertainty quantification","item":"https://shashankkroy.github.io/research-statement/neural-data-assimilation/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A neural network model for data assimilation and uncertainty quantification | S.K.R","name":"A neural network model for data assimilation and uncertainty quantification","description":"Can we perform data assimilation and uncertainty quantification using neural networks?","keywords":["Variational data assimilation, machine learning, neural networks, uncertainty quantification, AI","Deeplearning for weather predictions"],"wordCount":"1596","inLanguage":"en","datePublished":"2025-07-22T00:00:00Z","dateModified":"2025-07-22T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shashankkroy.github.io/research-statement/neural-data-assimilation/"},"publisher":{"@type":"Organization","name":"S.K.R","logo":{"@type":"ImageObject","url":"https://shashankkroy.github.io/favicon.ico"}}}</script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script>
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
</head>
<body class="type-research statement kind-page layout-" id=top><script data-no-instant>function switchTheme(a){switch(a){case'light':document.body.classList.remove('dark');break;case'dark':document.body.classList.add('dark');break;default:window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(a){switchTheme(a),localStorage.setItem("pref-theme",a)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=a=>{a?setPrefTheme('light'):setPrefTheme('dark')},window.addEventListener('toggle-theme',function(){const a=isDarkTheme();for(const b in toggleThemeCallbacks)toggleThemeCallbacks[b](a)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent('toggle-theme'))}</script>
<script>(function(){const b='auto',a=getPrefTheme(),c=a||b;switchTheme(c)})()</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://shashankkroy.github.io/ accesskey=h title="S.K.R (Alt + H)">S.K.R</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://shashankkroy.github.io/about/ title="About Me">About Me
</a>
</li>
<li>
<a href=https://shashankkroy.github.io/research-statement/ title="Research Statement" class=active>Research Statement
</a>
</li>
<li>
<a href=https://shashankkroy.github.io/curriculum-vitae/ title=CV>CV
</a>
</li>
<li>
<a href=https://shashankkroy.github.io/gallery/ title=Gallery>Gallery
</a>
</li>
<li>
<a href=https://shashankkroy.github.io/posts/ title=Posts>Posts
</a>
</li>
<li>
<a href=https://shashankkroy.github.io/tags/ title=tags>tags
</a>
</li>
<li>
<a href=https://shashankkroy.github.io/categories/ title=categories>categories
</a>
</li>
</ul>
</nav>
</header>
<main class="main post">
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://shashankkroy.github.io/>Home</a></div><h1 class=post-title>A neural network model for data assimilation and uncertainty quantification</h1>
<div class=post-description>Can we perform data assimilation and uncertainty quantification using neural networks?</div>
<div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>July 22, 2025</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://shashankkroy.github.io/tags/variational-data-assimilation-machine-learning-neural-networks-uncertainty-quantification-ai/>Variational data assimilation, machine learning, neural networks, uncertainty quantification, AI</a><a href=https://shashankkroy.github.io/tags/deeplearning-for-weather-predictions/>Deeplearning for weather predictions</a></span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>1596 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>8 min</span></span>
</div>
</header> <div class="toc side right">
<details open>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#neural-data-assimilation-using-4dvarnet>Neural Data Assimilation using 4DVarNet</a></li>
<li><a href=#problem-statement>Problem statement:</a></li>
<li><a href=#introducing-neural-data-assimilation-using-4dvarnet>Introducing Neural Data Assimilation using 4DVarNet</a></li>
<li><a href=#an-schematic-of-the-4dvarnet-architecture>An schematic of the 4DvarNet Architecture</a>
<ul>
<li><a href=#training-and-inference>Training and inference:</a></li>
</ul>
</li>
<li><a href=#quasi-geostrophic-model-our-choice-of-the-underlying-system>Quasi-geostrophic model: our choice of the underlying system</a></li>
<li><a href=#results-work-in-progress>Results (Work in progress)</a>
<ul>
<li><a href=#references>References</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><h2 id=neural-data-assimilation-using-4dvarnet>Neural Data Assimilation using 4DVarNet<a hidden class=anchor aria-hidden=true href=#neural-data-assimilation-using-4dvarnet>¶</a></h2>
<p><a href=https://www.ecmwf.int/en/research/data-assimilation>Data assimilation</a> algorithms are necessary to track or estimate the hidden state of chaotic systems through partial and noisy observations.
<a href=https://link.springer.com/chapter/10.1007/978-3-030-96709-3_5>Variational data assimilation</a> aims to find a the best trajectory of the dynamical system which minimizes a certain cost function.</p>
<p>This repositiory contains a new code for solving the weak-constraint 4dvar or simply weak-4dvar data assimilation for a differentiable QG model. All the code in this repo is implemented in pytorch with handing experiment configurations using hydra.</p>
<h2 id=problem-statement>Problem statement:<a hidden class=anchor aria-hidden=true href=#problem-statement>¶</a></h2>
<p></p>
<p>Given the sequence of observations Observations sequence $Y^i=\left(y^i_0,y^i_1,&mldr;y^i_n\right)$ on $\left(\Omega^i_n\right) \in \Omega$, find the optimal trajectory $X^i=\left(x^i_0,x^i_1,&mldr;x^i_n\right)$ that minimizes the following cost function.
The weak-4dvar cost function is:</p>
<p>$$\mathcal{U}(x^i_0,x^i_1,&mldr;x^i_n)=\sum_{k=1} \alpha_{dyn} | x_k - \mathcal{M}(x_{k-1}) |^2+ \alpha_{ob} |y_i-\mathcal{H}(x_i)|^2$$</p>
<p>The dynamical systems $ \mathcal{M} $, the dynamical propagator which takes the system state $x_k$ to $x_{k+1}$.
The above weak formulation of the 4dvar problem accounts for additional model errors in the dynamical system as the dynamics is not perfect, hence there dynamical cost term.</p>
<p>The first term minimizes the depatures from a pure model trajectory since the aim to find a trajectory close to the model trajectory while accounting for the model error- a part we refer to as &lsquo;the dynamical cost&rsquo;. The second term makes the trajectory fit to the obsrvations while accounting for the observation error and is referred as &lsquo;observation cost&rsquo;.</p>
<h2 id=introducing-neural-data-assimilation-using-4dvarnet>Introducing Neural Data Assimilation using 4DVarNet<a hidden class=anchor aria-hidden=true href=#introducing-neural-data-assimilation-using-4dvarnet>¶</a></h2>
<p><em>What if we have a dataset of the optimal states and their corresponding observations, can we train a neural network to learn the mapping from the observations to the optimal state?</em></p>
<p>But we know that a simple inversion of the observations to the full state is an ill-defined problem in data assimilation. For this, we generally use some kind of prior term such as a background term for initial guess field. This acts like a tikonov reguarlization term.</p>
<p>In the supervised setting, we something similar happens when we have access to a large dataset. Now when we have a dataset to learn from as in Supervised learning, we might not need a prior exactly like in 4dvar.</p>
<p>4DVarNet is trained to learn the mapping from the observations to the optimal state, while also learning representation term via a term called GIBBS neural network. This is an autoencoder based network whose input and outputs are the same.</p>
<p>Think of this part like an energy minization network that once minimized, learns a representation of the data which it then uses to perform the 4dvar cost minimization. The neural network is trained to minimize the cost function similar to the 4dvar cost function, but in a supervised manner.</p>
<h2 id=an-schematic-of-the-4dvarnet-architecture>An schematic of the 4DvarNet Architecture<a hidden class=anchor aria-hidden=true href=#an-schematic-of-the-4dvarnet-architecture>¶</a></h2>
<p>At the core of the 4VarNet architecture is a parameterized 4dvar cost function that we want to learn. Since the knowledge of the dynamics is not assumed, it learns a representation ot the data using a GIBBS neural network, and replaces the dynamical cost of the standard 4dvar DA. The GIBBS neural network is a type of neural network that learns a representation of the data by minimizing an energy function.</p>
<p>$$\mathcal{U}<em>\Phi= \alpha</em>{dyn} | X^i - \Phi(X^i) |^2+ \alpha_{ob} |Y^i-\mathcal{H}(X^i)|^2$$</p>
<p>Note, that this cost has two parts, the first part learns a representation of the data, while the second part ensures that the observations are matched to the state. There is no background term in this formulation, as the network is expected to learn the correlations from the data.</p>
<p></p>
<p>The solver of this cost function is another neural network that learns the mapping from the observations to the optimal state. This is parameterized by an LSTM whose input is a gradients. We can see this solver as doing meta-learning. The solver is trained to minimize the cost function using a gradient-based descent algorithm, which is similar to the optimization process used in 4dvar.</p>
<p>The parameterized cost function and it&rsquo;s solver are trained together in an end-to-end manner, allowing the network to learn the optimal mapping from the observations to the optimal state. The loss function that trains the whole architecture is the mean squared error between the predicted state and the true state, given by,</p>
<p>$$\mathcal{L}(X^i, \hat{X}^i) = \frac{1}{N} \sum_{i=1}^{N} | X^i - \Psi_{\Phi,\Gamma}(Y^i,{X}^0) |^2$$</p>
<p>where,<br>
$ \Phi, \Gamma,$ are the parameters of the GIBBS neural network and the solver neural network respectively, $Y^i$ is the observations over an assimilation window indexed by $i$, ${X}^0$ is the initial guess of the state, and $\Psi_{\Gamma,\Phi}$ is the mapping learned by the 4DVarNet architecture.</p>
<p>Interestingly, we just need observations and the true states, not the optimization solutions since the goal of the optimization itself is to revove the true field within the limits of the model/observation errors.</p>
<h3 id=training-and-inference>Training and inference:<a hidden class=anchor aria-hidden=true href=#training-and-inference>¶</a></h3>
<p>We have $(Y^i, X^i)$ define as above for the input and target pair for training. The input to the network is generally $Y^i$ in the standard architecture of 4DVarNet. In my case, the network starts with an initial guess of the state $X^i_0$ which is then used to compute the model trajectory $X^i$.</p>
<p>Usually, the IC can be a zero field or a random field, but in my case, I generate a slightly perturbed version of the true state $X^i_0$ as the initial guess. This is done by displacing the true state field in a coherent manner such that the features are preserved and are physical.</p>
<p>During inference, we can use the trained network to predict the optimal state given the observations and some initial conditions. Inprinciple, we can generate multiple predictions by sampling from the initial condition distribution, which can be a Gaussian distribution with mean as the initial guess and some variance. This gives us an ensemble of predictions which can be used for uncertainty quantification.</p>
<h2 id=quasi-geostrophic-model-our-choice-of-the-underlying-system>Quasi-geostrophic model: our choice of the underlying system<a hidden class=anchor aria-hidden=true href=#quasi-geostrophic-model-our-choice-of-the-underlying-system>¶</a></h2>
<p>To rigorously evaluate and benchmark state estimation algorithms, it is crucial to move beyond over-simplified dynamical systems like the Lorenz-63 or Lorenz-96 models, which are low-dimensional ODEs (3 and 40 dimensions, respectively). At the same time, we need models that are computationally tractable and can be used to test the performance of data assimilation algorithms.</p>
<p>With recent trends of deep learning, traditional data assimilation problems that need adjoint computation may bypass this step by leveraging AD(automatic differentiation)- the work-horse of modern machine learning. The Quasi-Geostrophic (QG) model offers a more realistic and challenging alternative. As a PDE-based model, the QG system captures essential features of large-scale geophysical fluid dynamics while remaining computationally tractable. It serves as a model of intermediate complexity, bridging the gap between toy models and full-scale numerical weather prediction systems.</p>
<p>In the QG model, the vorticity field is the fundamental dynamical variable, evolving under nonlinear advection and forcing, and governed by conservation laws. Observations, however, are typically taken in the streamfunction space, which is related to vorticity through an elliptic inversion (a form of diagnostic relationship). This setup introduces a realistic observation operator and offers a natural framework for exploring the performance of data assimilation techniques in the presence of model and observation noise. The QG model is thus particularly well-suited for testing advanced machine learning and deep learning methods for data assimilation I started out with the codes of <a href=https://github.com/hrkz/torchqg>Hugo Frezat</a> for QG, but I have made significant changes to the code to make it compatible with the weak-4dvar problem and to integrate it with neural ode package.</p>
<p>The Domain is 2D periodic, $\left[0,2 \pi\right)^2$, $\omega $: Vorticity field, $\psi$: Stream-function field.</p>
<p>$$
\frac{\partial \omega}{\partial t}+ J(\psi, \omega)= \nu \nabla^2 \omega - \mu \omega - \beta \partial_x \psi ,, \quad \omega = \nabla^2 \psi , \quad $$</p>
<p>The current implementation has Quasi-geostrophic model on a grid of $128$ X $128$ dimension. The ground truth is a $1024$ X $1024$ simulated voriticity and stream function coarse grained on the low resolution grid. The PDE has PBC and we use psueod-spectral methods to solve them( all in pytorch.)</p>
<p></p>
<p>These observations are available on masks which have been obtained from Nadir Satelite altimetry tracks. The nature of these observations are realistic- they are really quite sparse!
Below is a snapshot of vorticity field, the stream function and the kind of observations we have for any time. The observations and the masks are different at different times.</p>
<p>We finally create a dataset of observations and the true states, which can be used to train the 4DVarNet architecture. The dataset is created by simulating the QG model and generating observations at different times. The observations are then used to train the 4DVarNet architecture to learn the mapping from the observations to the true states.</p>
<h2 id=results-work-in-progress>Results (Work in progress)<a hidden class=anchor aria-hidden=true href=#results-work-in-progress>¶</a></h2>
<p>The standard 4DVarNet architecture only uses the observations as the input, and intializes the states from it. We refer to this as 4Dvarnet-No-IC. The 4DVarNet architecture where we provide additional guess for the initial conditions is referred to as 4DVarNet-IC. Using a simple metric called $\mu=1-\frac{RMSE(X)}{RMS(X)}$. The RMSE is the root mean square error between the predicted state and the true state, and RMS is the std. of the true state. When $\mu=1$, the model is perfect, and when $\mu=0$, the errors are of the same order of the natural variability of the field X.</p>
<p></p>
<p>The results of the 4DVarNet architecture are compared with the standard weak-4DVar algorithm, ( both solve the weak-4dvar problem).
The results show that the 4DVarNet-IC architecture performs better than the 4DVarNet-No-IC architecture, which is expected since the initial guess is closer to the true state. The results also show that the 4DVarNet architecture is able to learn the mapping from the observations to the true states, and can be used for data assimilation and uncertainty quantification.</p>
<h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>¶</a></h3>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=https://shashankkroy.github.io/research-statement/clv/>
<span class=title>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span>
<br>
<span>Covariant lyapunov vectors: Why are they interesting?</span>
</a>
<a class=next href=https://shashankkroy.github.io/research-statement/filter-stability/>
<span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg>
</span>
<br>
<span>Numerical filter stability: Do wrong priors at t=0 affect long term posterior?</span>
</a>
</nav>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2025 <a href=https://shashankkroy.github.io/>S.K.R</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span>
<span style=display:inline-block;margin-left:1em>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>(function(){const b=''=='1';if(b)return;let a=document.getElementById("theme-toggle");a.removeEventListener('click',toggleThemeListener),a.addEventListener('click',toggleThemeListener)})()</script>
<script>(function(){let a=document.getElementById('menu');a&&(a.scrollLeft=localStorage.getItem("menu-scroll-position"),a.onscroll=function(){localStorage.setItem("menu-scroll-position",a.scrollLeft)});const b=''=='1',c=''=='1';if(window.matchMedia('(prefers-reduced-motion: reduce)').matches||b||c)return;document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})})()</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>if(window.scrollListeners)for(const a of scrollListeners)window.removeEventListener('scroll',a);window.scrollListeners=[]</script>
<script src=/js/medium-zoom.min.js data-no-instant></script>
<script>(function(){const h='1'=='1';if(!h)return;if(!document.querySelector('.toc')){console.log('no toc found, ignore toc scroll');return}const i=window.scrollListeners,c=document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]'),d='active';let a=c[0];e(a).classList.add(d);const g=()=>{const b=[];for(const a of c)if(j(a)<5)b.push(a);else break;b.length>0?newActiveHeading=b[b.length-1]:newActiveHeading=c[0],a!=newActiveHeading&&(e(a).classList.remove(d),a=newActiveHeading,e(a).classList.add(d))};let b=null;const f=()=>{b!==null&&clearTimeout(b),b=setTimeout(g,50)};window.addEventListener('scroll',f,!1),i.push(f);function e(a){const b=encodeURI(a.getAttribute('id')).toLowerCase();return document.querySelector(`.toc ul li a[href="#${b}"]`)}function j(a){if(!a.getClientRects().length)return 0;let b=a.getBoundingClientRect();return b.top}})()</script>
</body>
</html>